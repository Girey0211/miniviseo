"""
AI Personal Assistant - HTTP API Server
"""
import sys
from pathlib import Path
from contextlib import asynccontextmanager

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import AsyncOpenAI

from parser.request_parser import parse_request
from router.agent_router import route_to_agent, register_agent
from mcp.client import get_mcp_client, register_tool
from config import OPENAI_API_KEY, OPENAI_MODEL
from utils.logger import get_logger

# Import agents
from agents.note_agent import NoteAgent
from agents.calendar_agent import CalendarAgent
from agents.web_agent import WebAgent
from agents.fallback_agent import FallbackAgent

# Import MCP tools
from mcp.tools import notes, http_fetcher, notion_calendar, notion_notes

logger = get_logger()

# Global instances
_mcp_client = None
_llm_client = None
_agent_instances = {}


def initialize_app():
    """Initialize MCP client, LLM client, and register agents/tools"""
    global _mcp_client, _llm_client, _agent_instances
    
    logger.info("Initializing AI Personal Assistant API Server...")
    
    # Initialize MCP client
    _mcp_client = get_mcp_client()
    logger.debug("MCP client initialized")
    
    # Register MCP tools
    register_tool("notes", notes)
    register_tool("http_fetcher", http_fetcher)
    register_tool("notion_calendar", notion_calendar)
    register_tool("notion_notes", notion_notes)
    logger.info("MCP tools registered")
    
    # Initialize LLM client
    _llm_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    logger.debug(f"LLM client initialized with model: {OPENAI_MODEL}")
    
    # Create agent instances
    _agent_instances = {
        "NoteAgent": NoteAgent(mcp_client=_mcp_client, llm_client=_llm_client),
        "CalendarAgent": CalendarAgent(mcp_client=_mcp_client, llm_client=_llm_client),
        "WebAgent": WebAgent(mcp_client=_mcp_client),
        "FallbackAgent": FallbackAgent(mcp_client=_mcp_client, llm_client=_llm_client),
    }
    
    # Register agents with router
    for agent_name, agent_instance in _agent_instances.items():
        register_agent(agent_name, agent_instance)
    
    logger.info(f"Agents registered: {', '.join(_agent_instances.keys())}")
    logger.info("API Server initialization complete")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup/shutdown"""
    # Startup
    initialize_app()
    yield
    # Shutdown
    logger.info("Shutting down API Server...")


# Create FastAPI app
app = FastAPI(
    title="AI Personal Assistant API",
    description="""
    ## LLM ê¸°ë°˜ ê°œì¸ ë¹„ì„œ API
    
    ìì—°ì–´ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ### ì§€ì› ê¸°ëŠ¥
    - ğŸ“ ë©”ëª¨ ì‘ì„± ë° ì¡°íšŒ (Notion í†µí•©)
    - ğŸ“… ì¼ì • ê´€ë¦¬ (Notion í†µí•©)
    - ğŸ” ì›¹ ê²€ìƒ‰ ë° ìš”ì•½
    
    ### ì‚¬ìš© ë°©ë²•
    1. `/assistant` ì—”ë“œí¬ì¸íŠ¸ì— POST ìš”ì²­
    2. JSON bodyì— `text` í•„ë“œë¡œ ìì—°ì–´ ìš”ì²­ ì „ë‹¬
    3. ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬ ê²°ê³¼ ìˆ˜ì‹ 
    
    ### ì˜ˆì‹œ
    ```json
    {
      "text": "ì˜¤ëŠ˜ í•œ ì¼ ë©”ëª¨í•´ì¤˜: í”„ë¡œì íŠ¸ ì™„ë£Œ"
    }
    ```
    """,
    version="0.1.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response models
class AssistantRequest(BaseModel):
    """ìì—°ì–´ ìš”ì²­"""
    text: str
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "text": "ì˜¤ëŠ˜ í•œ ì¼ ë©”ëª¨í•´ì¤˜: í”„ë¡œì íŠ¸ ì™„ë£Œ"
                },
                {
                    "text": "ë‚´ì¼ ì˜¤í›„ 3ì‹œì— íŒ€ íšŒì˜ ì¶”ê°€í•´ì¤˜"
                },
                {
                    "text": "íŒŒì´ì¬ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰í•´ì¤˜"
                }
            ]
        }
    }


class AssistantResponse(BaseModel):
    """ì²˜ë¦¬ ê²°ê³¼ ì‘ë‹µ"""
    response: str
    intent: str
    agent: str
    status: str
    
    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "response": "ë©”ëª¨ë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.",
                    "intent": "write_note",
                    "agent": "NoteAgent",
                    "status": "ok"
                },
                {
                    "response": "ì¼ì •ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.",
                    "intent": "calendar_add",
                    "agent": "CalendarAgent",
                    "status": "ok"
                }
            ]
        }
    }


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ"""
    status: str
    version: str


async def summarize_result(result: dict, parsed_request) -> str:
    """
    Generate natural language response from agent result using LLM
    
    Args:
        result: Result dictionary from agent
        parsed_request: Original parsed request
        
    Returns:
        Natural language response string
    """
    if result.get("status") == "error":
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
    
    # Create prompt for LLM to generate natural response
    prompt = f"""ì‚¬ìš©ìì˜ ìš”ì²­: "{parsed_request.raw_text}"
Intent: {parsed_request.intent}
ì‹¤í–‰ ê²°ê³¼: {result.get('result')}

ìœ„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
- ê²°ê³¼ì˜ í•µì‹¬ ì •ë³´ë¥¼ í¬í•¨
- ì¹œê·¼í•œ í†¤ ì‚¬ìš©"""

    try:
        response = await _llm_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ê°œì¸ ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ê°„ê²°í•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Error in summarize_result: {str(e)}")
        return f"ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼: {result.get('result')}"


@app.get("/", response_model=HealthResponse, tags=["Health"])
async def root():
    """
    ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
    
    ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    return HealthResponse(status="ok", version="0.1.0")


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health():
    """
    í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    
    ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    return HealthResponse(status="ok", version="0.1.0")


@app.post("/assistant", response_model=AssistantResponse, tags=["Assistant"])
async def process_request(request: AssistantRequest):
    """
    ìì—°ì–´ ìš”ì²­ ì²˜ë¦¬
    
    ìì—°ì–´ë¡œ ì‘ì„±ëœ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ Agentë¥¼ í†µí•´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    ## ì§€ì›í•˜ëŠ” ìš”ì²­ ìœ í˜•
    
    ### ë©”ëª¨ ì‘ì„±
    - "ì˜¤ëŠ˜ í•œ ì¼ ë©”ëª¨í•´ì¤˜: í”„ë¡œì íŠ¸ ì™„ë£Œ"
    - "íšŒì˜ë¡ ì‘ì„±í•´ì¤˜: íŒ€ ë¯¸íŒ… ë‚´ìš©"
    
    ### ë©”ëª¨ ì¡°íšŒ
    - "ë‚´ ë©”ëª¨ ëª©ë¡ ë³´ì—¬ì¤˜"
    - "ë©”ëª¨ ë¦¬ìŠ¤íŠ¸ ì•Œë ¤ì¤˜"
    
    ### ì¼ì • ì¶”ê°€
    - "ë‚´ì¼ ì˜¤í›„ 3ì‹œì— íŒ€ íšŒì˜ ì¶”ê°€í•´ì¤˜"
    - "ë‹¤ìŒì£¼ ì›”ìš”ì¼ ì˜¤ì „ 10ì‹œì— ë°œí‘œ ì¼ì • ì¡ì•„ì¤˜"
    
    ### ì¼ì • ì¡°íšŒ
    - "ì´ë²ˆ ì£¼ ì¼ì • ë³´ì—¬ì¤˜"
    - "ì˜¤ëŠ˜ ì¼ì • ì•Œë ¤ì¤˜"
    
    ### ì›¹ ê²€ìƒ‰
    - "íŒŒì´ì¬ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰í•´ì¤˜"
    - "OpenAI API ë¬¸ì„œ ì°¾ì•„ì¤˜"
    
    ## ì‘ë‹µ í˜•ì‹
    
    - **response**: ìì—°ì–´ë¡œ ì‘ì„±ëœ ì‘ë‹µ ë©”ì‹œì§€
    - **intent**: íŒŒì‹±ëœ ì˜ë„ (write_note, list_notes, calendar_add, calendar_list, web_search ë“±)
    - **agent**: ìš”ì²­ì„ ì²˜ë¦¬í•œ Agent ì´ë¦„
    - **status**: ì²˜ë¦¬ ìƒíƒœ (ok ë˜ëŠ” error)
    """
    try:
        logger.info(f"API request received: {request.text}")
        
        # Step 1: Parse request
        parsed = await parse_request(request.text)
        logger.debug(f"Parsed - Intent: {parsed.intent}, Agent: {parsed.agent}")
        
        # Step 2: Route to agent
        agent_class = route_to_agent(parsed)
        
        if agent_class is None:
            logger.warning("No agent found for request")
            raise HTTPException(status_code=400, detail="ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # Get agent instance
        agent = _agent_instances.get(parsed.agent)
        
        if agent is None:
            logger.warning(f"Agent {parsed.agent} not found, using FallbackAgent")
            agent = _agent_instances.get("FallbackAgent")
            
            if agent is None:
                logger.error("FallbackAgent not available")
                raise HTTPException(status_code=500, detail="Agent not available")
        
        logger.info(f"Routing to agent: {agent.get_agent_name()}")
        
        # Step 3: Execute agent
        params_with_intent = {**parsed.params, "intent": parsed.intent}
        result = await agent.handle(params_with_intent)
        logger.debug(f"Agent result: {result.get('status')}")
        
        # Step 4: Generate natural language response
        final_response = await summarize_result(result, parsed)
        logger.info("Request processed successfully")
        
        return AssistantResponse(
            response=final_response,
            intent=parsed.intent,
            agent=parsed.agent,
            status=result.get("status", "ok")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


def main():
    """Entry point for server"""
    import uvicorn
    import signal
    
    def signal_handler(sig, frame):
        logger.info("Received shutdown signal, stopping server...")
        sys.exit(0)
    
    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    logger.info("Starting AI Personal Assistant API Server...")
    logger.info("Swagger UI: http://0.0.0.0:8000/docs")
    logger.info("ReDoc: http://0.0.0.0:8000/redoc")
    logger.info("Press Ctrl+C to stop")
    
    try:
        uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
    except KeyboardInterrupt:
        logger.info("Server stopped by user")


if __name__ == "__main__":
    main()
